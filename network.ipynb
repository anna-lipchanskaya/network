{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Скачивание dataset"
      ],
      "metadata": {
        "id": "J03Z3o_R8hdc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3xP44VzkXnu"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"nibinv23/iam-handwriting-word-database\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка корректности путей"
      ],
      "metadata": {
        "id": "tCJ0RP6m82CE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Скачиваем датасет\n",
        "path = kagglehub.dataset_download(\"nibinv23/iam-handwriting-word-database\")\n",
        "print(\"Путь к датасету:\", path)\n",
        "\n",
        "# Проверяем содержимое папки\n",
        "print(\"Файлы в датасете:\", os.listdir(path))"
      ],
      "metadata": {
        "id": "EdmopM50XHS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_dict = {}\n",
        "\n",
        "with open(\"/root/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2/words_new.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        # Пропускаем строки, начинающиеся с #\n",
        "        if line.startswith(\"#\"):\n",
        "            continue\n",
        "\n",
        "        parts = line.strip().split()\n",
        "\n",
        "        # Пропускаем строки с ошибками сегментации\n",
        "        if parts[1] != \"ok\":\n",
        "            continue\n",
        "\n",
        "        # Берём первый элемент как ключ и последний как значение\n",
        "        word_id = parts[0]\n",
        "        word = parts[-1]\n",
        "\n",
        "        word_dict[word_id] = word\n"
      ],
      "metadata": {
        "id": "a2Tzu5BZl-is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим, что получилось\n",
        "print(list(word_dict.items())[:100])  # Выведет первые 10 элементов словаря"
      ],
      "metadata": {
        "id": "pcKFU0OrmM4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все нужные импорты"
      ],
      "metadata": {
        "id": "gQSNcCIK9jW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.backend import get_value, ctc_decode\n",
        "from keras import Model\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Bidirectional, Reshape, LeakyReLU, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "AcpGCLZDmPX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Код для обработки изображений"
      ],
      "metadata": {
        "id": "8zeA7sni9nZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def adaptive_threshold(image):\n",
        "    # Применим фильтр Гаусса\n",
        "    blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    # Применение адаптивной бинаризации\n",
        "    binary = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_MEAN_C,\n",
        "                                 cv2.THRESH_BINARY, 21, 10)\n",
        "\n",
        "    # Инверсия цветов\n",
        "    inverted = cv2.bitwise_not(binary)\n",
        "\n",
        "    return inverted\n",
        "\n",
        "def normalize(image):\n",
        "    normalized = image.astype(np.float32) / 255.0\n",
        "\n",
        "    return normalized\n",
        "\n",
        "def resize_and_reshape(image, target_size=(64, 200)):\n",
        "    # Конвертация в grayscale\n",
        "    if len(image.shape) == 3:\n",
        "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        gray = image.copy()\n",
        "\n",
        "    # Изменение размера\n",
        "    h, w = gray.shape\n",
        "    if h > target_size[0] or w > target_size[1]:\n",
        "        shrink = min(target_size[0]/h, target_size[1]/w)\n",
        "        resized = cv2.resize(gray, None, fx=shrink, fy=shrink,\n",
        "                            interpolation=cv2.INTER_AREA)\n",
        "    else:\n",
        "        resized = gray.copy()\n",
        "\n",
        "    # Добавление рамок\n",
        "    pad_h = target_size[0] - resized.shape[0]\n",
        "    pad_w = target_size[1] - resized.shape[1]\n",
        "    padded = cv2.copyMakeBorder(resized,\n",
        "                               math.ceil(pad_h/2), math.floor(pad_h/2),\n",
        "                               math.ceil(pad_w/2), math.floor(pad_w/2),\n",
        "                               cv2.BORDER_CONSTANT, value=255)\n",
        "\n",
        "    # Поворот\n",
        "    rotated = cv2.rotate(padded, cv2.ROTATE_90_CLOCKWISE)\n",
        "\n",
        "    return rotated\n",
        "\n",
        "def preprocess_image(image):\n",
        "    resized = resize_and_reshape(image)\n",
        "\n",
        "    thresholded = adaptive_threshold(resized)\n",
        "\n",
        "    normalized = normalize(thresholded)\n",
        "\n",
        "    return normalized"
      ],
      "metadata": {
        "id": "bBxDCOpcntIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функции для закодирования и разкодирования"
      ],
      "metadata": {
        "id": "DFy0NhIm9rfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_label(num, alphabet):\n",
        "    return ''.join(alphabet[ch] for ch in num if ch != len(alphabet))\n",
        "\n",
        "def decode_predictions(nums):\n",
        "    input_length = np.ones(nums.shape[0]) * nums.shape[1]\n",
        "    values = get_value(\n",
        "        ctc_decode(nums, input_length=input_length, greedy=True)[0][0]\n",
        "    )\n",
        "\n",
        "    texts = [decode_label(value[value >= 0], alphabet) for value in values]\n",
        "    return texts\n",
        "\n",
        "\n",
        "def encode_labels(texts):\n",
        "    def _decode_label(label, alphabet):\n",
        "        return np.array([alphabet.find(ch) for ch in label])\n",
        "\n",
        "    # Создание алфавита из текстов\n",
        "    alphabet = ''.join(sorted(set(''.join(texts))))\n",
        "\n",
        "    max_len = max(len(text) for text in texts)\n",
        "    nums = np.full((len(texts), max_len), fill_value=len(alphabet), dtype='int64')\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        nums[i, :len(text)] = _decode_label(text, alphabet)\n",
        "\n",
        "    return nums, alphabet"
      ],
      "metadata": {
        "id": "ZEoL88ymnyxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для проверки изображений"
      ],
      "metadata": {
        "id": "cikCzabb9xGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_random_image(train_X, train_y, num_images=8):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for i in range(num_images):\n",
        "        random_index = random.randint(0, len(train_X) - 1)\n",
        "        random_image = train_X[random_index]\n",
        "        random_label = train_y[random_index]\n",
        "\n",
        "        plt.subplot(2, 4, i + 1)\n",
        "        plt.imshow(random_image, cmap='gray')\n",
        "        plt.title(f'Label: {random_label}')\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yMzJqhRvn1Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрика и функция потерь"
      ],
      "metadata": {
        "id": "E5NEHTnS9_En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharacterErrorRateCalculator(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='character_error_score', **kwargs):\n",
        "        super(CharacterErrorRateCalculator, self).__init__(name=name, **kwargs)\n",
        "        self.error_sum = self.add_weight(name=\"cumulative_errors\", initializer=\"zeros\")\n",
        "        self.num_samples = self.add_weight(name=\"sample_counter\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, ground_truth, predictions, sample_weight=None):\n",
        "        pred_shape = K.shape(predictions)\n",
        "        seq_len = tf.ones(shape=pred_shape[0]) * K.cast(pred_shape[1], 'float32')\n",
        "\n",
        "        decoded_output, _ = K.ctc_decode(predictions, seq_len, greedy=True)\n",
        "\n",
        "        sparse_decoded = K.ctc_label_dense_to_sparse(decoded_output[0], K.cast(seq_len, 'int32'))\n",
        "        sparse_truth = K.ctc_label_dense_to_sparse(ground_truth, K.cast(seq_len, 'int32'))\n",
        "        sparse_truth = tf.sparse.retain(sparse_truth, tf.not_equal(sparse_truth.values, tf.math.reduce_max(sparse_truth.values)))\n",
        "\n",
        "        sparse_decoded = tf.sparse.retain(sparse_decoded, tf.not_equal(sparse_decoded.values, -1))\n",
        "        error_rate = tf.edit_distance(sparse_decoded, sparse_truth, normalize=True)\n",
        "\n",
        "        self.error_sum.assign_add(tf.reduce_sum(error_rate))\n",
        "        self.num_samples.assign_add(K.cast(ground_truth.shape[1], 'float32'))\n",
        "\n",
        "    def result(self):\n",
        "        return tf.math.divide_no_nan(self.error_sum, self.num_samples)\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.error_sum.assign(0.0)\n",
        "        self.num_samples.assign(0.0)\n",
        "\n",
        "def CTCLoss(targets, outputs):\n",
        "    batch_size = tf.cast(tf.shape(targets)[0], dtype=\"int64\")\n",
        "    output_seq_length = tf.cast(tf.shape(outputs)[1], dtype=\"int64\")\n",
        "    target_seq_length = tf.cast(tf.shape(targets)[1], dtype=\"int64\")\n",
        "\n",
        "    output_seq_length = output_seq_length * tf.ones(shape=(batch_size, 1), dtype=\"int64\")\n",
        "    target_seq_length = target_seq_length * tf.ones(shape=(batch_size, 1), dtype=\"int64\")\n",
        "\n",
        "    calculated_loss = K.ctc_batch_cost(targets, outputs, output_seq_length, target_seq_length)\n",
        "    return calculated_loss"
      ],
      "metadata": {
        "id": "IyIa20X49-1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Архитектура нейронной сети"
      ],
      "metadata": {
        "id": "hcuxVcE--F9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout\n",
        "def create_model(num_classes, input_shape=(200, 64, 1)):\n",
        "    input_img = Input(shape=input_shape, name='image_input')\n",
        "\n",
        "    # Сверточные слои с нормализацией\n",
        "    x = Conv2D(64, (5, 5), padding='same')(input_img)\n",
        "    x = LeakyReLU(0.01)(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (5, 5), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.01)(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.01)(x)\n",
        "    x = MaxPooling2D((1, 2))(x)\n",
        "\n",
        "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.01)(x)\n",
        "\n",
        "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = LeakyReLU(0.01)(x)\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "    # Reshape для LSTM слоев\n",
        "    x = Reshape((25, -1))(x)\n",
        "\n",
        "    # Полносвязный слой с Dropout\n",
        "    x = Dense(256)(x)\n",
        "    x = LeakyReLU(0.01)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Рекуррентные слои\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
        "\n",
        "    # Полносвязный слой с Dropout\n",
        "    x = Dense(256)(x)\n",
        "    x = LeakyReLU(0.01)(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "\n",
        "    # Выходной слой\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=input_img, outputs=x)\n",
        "    model.compile(optimizer=Nadam(learning_rate=0.001, clipnorm=1.0), loss=CTCLoss, metrics=[CharacterErrorRateCalculator()])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Qzz32SGKn4IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для загрузки изображений и метрики"
      ],
      "metadata": {
        "id": "L8JbjnLS-Ju9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(word_dict):\n",
        "    image_base_folder = f'/root/.cache/kagglehub/datasets/nibinv23/iam-handwriting-word-database/versions/2/iam_words/words'\n",
        "    train_X, train_y = [], []\n",
        "    total_count = 0\n",
        "    folder_counts = {}\n",
        "    max_per_folder = 200\n",
        "    max_total = 15000\n",
        "\n",
        "    for root, _, files in os.walk(image_base_folder):\n",
        "        if total_count >= max_total:\n",
        "            break\n",
        "\n",
        "        folder_name = os.path.basename(root)\n",
        "        folder_counts[folder_name] = 0\n",
        "\n",
        "        for image_file in files:\n",
        "            if total_count >= max_total:\n",
        "                break\n",
        "\n",
        "            if folder_counts[folder_name] >= max_per_folder:\n",
        "                break\n",
        "\n",
        "            if image_file.endswith('.jpg') or image_file.endswith('.png'):\n",
        "                image_name = image_file.rsplit('.', 1)[0]\n",
        "\n",
        "                # Проверяем, есть ли имя в словаре\n",
        "                if image_name not in word_dict:\n",
        "                    continue  # Пропускаем изображение без метки\n",
        "\n",
        "                label = word_dict[image_name]\n",
        "                if label == 'Ps':\n",
        "                  continue\n",
        "\n",
        "                if not any(c not in string.ascii_letters + ',.\\' ' for c in label):\n",
        "                    image_path = os.path.join(root, image_file)\n",
        "                    if os.path.exists(image_path):\n",
        "                        try:\n",
        "                            img_array = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
        "                            if img_array is None:\n",
        "                                raise ValueError(\"Failed to read image with OpenCV\")\n",
        "\n",
        "                            if len(label) <= 12:\n",
        "                                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)\n",
        "                                train_X.append(preprocess_image(img_array))\n",
        "                                train_y.append(label)\n",
        "                                folder_counts[folder_name] += 1\n",
        "                                total_count += 1\n",
        "                        except Exception as e:\n",
        "                            print(f\"Failed to load image: {image_path}, error: {e}\")\n",
        "                    else:\n",
        "                        print(f\"File does not exist: {image_path}\")\n",
        "\n",
        "    return np.array(train_X), np.array(train_y)\n"
      ],
      "metadata": {
        "id": "YHTSjIV8oPG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Обработка данных для train и test\n",
        "train_X, train_y = process_data(word_dict)\n",
        "train_y, alphabet = encode_labels(train_y)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Azr1x3o_n8LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример вывода данных\n",
        "check_random_image(train_X, train_y)\n",
        "# Разбиваем данные: 80% - train+val, 20% - test\n",
        "train_X, test_X, train_y, test_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Разбиваем train+val: 80% - train, 20% - val (итого: 64% train, 16% val, 20% test)\n",
        "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Подготовка данных окончена\")"
      ],
      "metadata": {
        "id": "96OWulbBpsR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализация нейронной сети"
      ],
      "metadata": {
        "id": "yKBBMZt8-qTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(alphabet) + 1  # Количество классов + 1 для CTC blank\n",
        "print(alphabet)\n",
        "print(f\"Форма y_true: {train_y.shape}\")\n",
        "print(num_classes)\n",
        "input_shape = (200, 64, 1)\n",
        "model = create_model(num_classes, input_shape)"
      ],
      "metadata": {
        "id": "D2etQKIlp0MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение"
      ],
      "metadata": {
        "id": "GqVuC7zU-1cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks (обратные вызовы)\n",
        "early_stopping = EarlyStopping(monitor='val_character_error_score',\n",
        "                              patience=10,\n",
        "                              restore_best_weights=True,\n",
        "                              mode='min')\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_character_error_score',\n",
        "                             factor=0.5,\n",
        "                             min_lr=1e-5,\n",
        "                             patience=4,\n",
        "                             mode='min')\n",
        "history = model.fit(\n",
        "  train_X, train_y,\n",
        "  validation_data=(val_X, val_y),\n",
        "  epochs=60,\n",
        "  batch_size=64,\n",
        "  callbacks=[early_stopping, reduce_lr],\n",
        "  verbose=1\n",
        "    )"
      ],
      "metadata": {
        "id": "CH2pCbrap-Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохранение модели"
      ],
      "metadata": {
        "id": "TGxnLpnE-3QD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "model.save('handwritten.keras')\n",
        "files.download('handwritten.keras')\n"
      ],
      "metadata": {
        "id": "VACF_EU7x-xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Вывод метрики и функции потерь при обучении"
      ],
      "metadata": {
        "id": "51J1i8d0-5JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(figsize=(12, 6), ncols=2, nrows=1)\n",
        "\n",
        "axes[0].plot(history.history['loss'], label='loss')\n",
        "axes[0].plot(history.history['val_loss'], label='val_loss')\n",
        "axes[0].set_xlabel(\"Epoch\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].legend()\n",
        "axes[1].plot(history.history['character_error_score'], label='character_error_score')\n",
        "axes[1].plot(history.history['val_character_error_score'], label='val_character_error_score')\n",
        "axes[1].set_xlabel(\"Epoch\")\n",
        "axes[1].set_ylabel(\"Character Error Score\")\n",
        "axes[1].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tMyxGjfcyVnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Оценка модели на тестовом наборе данных\n",
        "loss, accuracy = model.evaluate(test_X, test_y)\n",
        "\n",
        "# Вывод значений потерь и точности\n",
        "print(f'Loss: {loss}')\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "id": "vZUI-V-HybQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод всех предсказанных символов с соответствующими изображениями"
      ],
      "metadata": {
        "id": "juW1TdfC_TJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Получение предсказаний модели\n",
        "predicts = model.predict(test_X)\n",
        "labels = decode_predictions(predicts)\n",
        "\n",
        "# Последовательный вывод изображений\n",
        "for idx in range(len(test_X)):\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(test_X[idx].astype(np.uint8), cmap='gray')\n",
        "\n",
        "    predicted_char = labels[idx]\n",
        "    gt_label = decode_label(test_y[idx], alphabet)\n",
        "\n",
        "    plt.title(f'GT: {gt_label} - Predict: {predicted_char}')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "j21sojRZyj0x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}